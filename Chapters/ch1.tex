\chapter{INTRODUCTION}

\section{Overview of Functional Magnetic Resonance Imaging (fMRI)}

\hspace{0.5cm}Functional Magnetic Resonance Imaging (fMRI) represents a revolutionary non-invasive neuroimaging technique that has transformed our understanding of human brain function since its development in the early 1990s. Unlike structural MRI which captures static anatomical information, fMRI measures dynamic brain activity by detecting changes in blood oxygenation levels associated with neural activity, a phenomenon known as the Blood-Oxygen-Level-Dependent (BOLD) signal.

The fundamental principle underlying fMRI is neurovascular coupling when neurons in a particular brain region become active, they require increased oxygen and glucose to support their metabolic demands. The brain responds by increasing blood flow to these active regions, delivering oxygen-rich hemoglobin. Since oxygenated and deoxygenated hemoglobin possess different magnetic properties, this localized change in blood oxygenation creates detectable alterations in the MRI signal. Modern fMRI scanners can capture these hemodynamic changes with spatial resolution of 2-3 millimeters and temporal resolution of 1-3 seconds, enabling researchers to observe which brain regions activate during specific cognitive, perceptual, or emotional processes.

fMRI has become the dominant tool in cognitive neuroscience research due to several key advantages. First, it provides whole-brain coverage, allowing simultaneous observation of distributed neural networks rather than isolated brain regions. Second, unlike Positron Emission Tomography (PET), fMRI does not require radioactive tracers, making it safe for repeated measurements on the same individuals. Third, the non-invasive nature permits studies of healthy populations across diverse age ranges and clinical conditions. These capabilities have enabled groundbreaking discoveries about brain organization, from mapping the default mode network during rest to revealing neural circuits underlying memory, attention, and emotion processing.

In the context of emotion research, fMRI has proven particularly valuable for identifying brain networks involved in affective processing. Traditional lesion studies and electrophysiology provided limited spatial coverage, while fMRI enables comprehensive mapping of emotion-related activations across cortical and subcortical structures. Researchers have used fMRI to demonstrate that emotions engage distributed networks involving the amygdala for salience detection, prefrontal cortex for regulation and appraisal, insula for interoceptive awareness, and anterior cingulate for conflict monitoring. These findings have shifted emotion theories away from localizationist models proposing dedicated emotion centers toward network-based frameworks emphasizing interactions across multiple brain systems.

However, fMRI data also presents substantial analytical challenges. Each scanning session generates high-dimensional datasets containing millions of voxel time series, creating a "curse of dimensionality" problem where the number of features vastly exceeds the number of observations. The BOLD signal exhibits low signal-to-noise ratio, requiring sophisticated preprocessing to remove physiological noise from cardiac and respiratory cycles, head motion artifacts, and scanner drift. Additionally, the hemodynamic response peaks 4-6 seconds after neural activity, introducing temporal lag between cognitive events and measured signals. These complexities necessitate advanced analytical approaches capable of extracting meaningful patterns from noisy, high-dimensional neuroimaging data-motivating the application of machine learning and deep learning techniques to fMRI analysis.

\section{Machine Learning Approaches in fMRI Analysis}

\hspace{0.5cm}The application of machine learning to fMRI analysis represents a paradigm shift from traditional univariate statistical methods toward multivariate pattern recognition approaches. Classical fMRI analysis employed General Linear Models (GLM) and t-tests to identify individual voxels showing significant activation differences between experimental conditions. While effective for localization, these mass-univariate methods discard information about spatial patterns and inter-voxel relationships, limiting their sensitivity for detecting distributed neural representations.

Machine learning methods, in contrast, leverage patterns across multiple voxels simultaneously, enabling decoding of mental states from distributed brain activity. The fundamental premise of brain decoding is that specific cognitive or emotional states correspond to characteristic spatial patterns of neural activity that machine learning classifiers can learn to recognize. Rather than asking "which brain regions activate during emotion X?", decoding approaches ask "can we identify which emotion a person is experiencing based on their brain activity pattern?" This predictive framework naturally aligns with machine learning's supervised learning paradigm.

Support Vector Machines (SVMs) emerged as the dominant classical approach for fMRI classification due to their effectiveness with high-dimensional data. SVMs construct optimal hyperplanes that maximally separate different classes in feature space, with kernel functions enabling nonlinear decision boundaries. The regularization inherent in SVMs prevents overfitting even when the number of features (voxels) exceeds the number of samples. Numerous studies have successfully applied SVMs to decode various mental states including visual stimuli categories, memory contents, and emotional experiences. For emotion classification specifically, SVMs typically extract features from anatomically-defined regions of interest or whole-brain voxel patterns, achieving accuracies ranging from 60-75\% for multi-class discrimination.

Random Forests represent another widely-used approach, combining predictions from multiple decision trees trained on bootstrap samples of the data. This ensemble method offers several advantages for neuroimaging: natural handling of high-dimensional inputs, robustness to outliers and noisy features, and built-in feature importance measures that identify which brain regions contribute most to classification. Random Forests have demonstrated competitive performance with SVMs while providing greater interpretability through feature importance rankings. Studies applying Random Forests to emotion decoding have revealed that limbic-prefrontal connections and default mode network patterns carry substantial discriminative information.

Logistic Regression, despite its simplicity, remains valuable as an interpretable baseline. L2-regularized logistic regression learns linear weights for each feature, enabling direct interpretation of which brain regions positively or negatively contribute to each emotion category. While typically achieving lower accuracy than nonlinear methods, logistic regression's transparency makes it useful for hypothesis testing and comparison against more complex models.

Beyond classification, machine learning has enabled novel analyses of fMRI data including representational similarity analysis, dimensionality reduction through Principal Component Analysis and Independent Component Analysis, and functional connectivity estimation. Connectivity analysis examines statistical relationships between regional time series, revealing network organization underlying cognitive functions. Pearson correlation between regions provides the simplest connectivity measure, though partial correlation and regularized precision estimation offer more sophisticated approaches accounting for indirect connections.

Despite these successes, classical machine learning methods face inherent limitations when applied to fMRI analysis. First, they typically require manual feature engineering-researchers must decide which brain regions to examine, how to summarize regional activity, and which connectivity measures to compute. This introduces researcher bias and may miss important patterns not anticipated a priori. Second, most classical methods employ shallow architectures with limited capacity for hierarchical representation learning. Complex brain patterns involving interactions across multiple spatial scales may exceed the representational capacity of linear or single-layer nonlinear models. Third, classical approaches struggle with very small sample sizes typical in neuroimaging studies (10-30 subjects), as they cannot leverage the temporal structure of fMRI time series to augment effective sample size.

These limitations motivated exploration of deep learning approaches that can automatically learn hierarchical feature representations directly from neuroimaging data, eliminating manual feature engineering while capturing complex nonlinear patterns that classical methods miss, thus promising more robust and accurate diagnostic and prognostic models. This transition leverages the power of convolutional and recurrent networks to discern subtle, clinically relevant biomarkers hidden within high-dimensional imaging datasets, significantly enhancing the potential for personalized medicine and early detection of neurological disorders, by offering a data-driven pathway to understand brain pathology. The capacity of deep learning to handle the inherent noise and variability in clinical data makes it an increasingly indispensable tool for translating raw scanner output into actionable insights.
\section{Deep Learning for Emotion Recognition in Brain Decoding}

\hspace{0.5cm}Deep learning has revolutionized computer vision, natural language processing, and speech recognition through its capacity to automatically learn hierarchical representations from raw data. Unlike classical machine learning requiring handcrafted features, deep neural networks discover increasingly abstract features through multiple processing layers from simple edges in early layers to complex object parts in intermediate layers to semantic concepts in deep layers. This automatic feature learning has proven transformative across domains, motivating its application to neuroimaging analysis including emotion recognition from fMRI.

Convolutional Neural Networks (CNNs), originally developed for image analysis, have emerged as a natural architecture for processing fMRI data. CNNs exploit spatial structure through convolutional layers that apply learnable filters across the input, detecting local patterns while sharing parameters to reduce model complexity. For fMRI analysis, brain activity patterns and connectivity matrices can be represented as structured 2D or 3D images amenable to convolutional processing. Several key architectural components make CNNs particularly suitable for neuroimaging: convolutional layers capture local spatial patterns, pooling layers provide translation invariance and spatial downsampling, batch normalization stabilizes training dynamics, dropout prevents overfitting, and fully-connected classification layers map learned features to emotion categories.

Recent studies have begun applying CNNs to fMRI-based emotion recognition with promising results. Early approaches processed whole-brain activation maps as 3D volumes, using 3D convolutions to detect spatial patterns across voxels. However, these methods face challenges from the extremely high dimensionality of volumetric fMRI data (often 100,000+ voxels) and small sample sizes typical in neuroimaging studies. More recent work has focused on functional connectivity matrices representing pairwise correlations between brain regions, which can be treated as 2D images with spatial structure reflecting anatomical relationships between connections.

The application of CNNs to connectivity matrices offers several advantages over classical approaches. First, CNNs automatically learn which connections and connection patterns discriminate emotions, eliminating the need for manual feature selection based on prior hypotheses. Second, hierarchical feature learning enables discovery of multi-scale patterns from individual connections in early layers to network motifs in intermediate layers to whole-brain configurations in deep layers. Third, CNNs naturally capture nonlinear relationships and interactions between features that linear models cannot represent. Fourth, weight sharing through convolution dramatically reduces the number of parameters compared to fully-connected networks, providing regularization that helps prevent overfitting on small datasets.

Despite their promise, applying deep learning to fMRI emotion recognition presents substantial technical challenges. The primary challenge is small sample size: while CNNs excel with millions of training examples in computer vision, neuroimaging studies typically include only 10-30 subjects due to scanning costs and time constraints. This creates severe risk of overfitting where models memorize training examples rather than learning generalizable patterns. Researchers have addressed this through several strategies: aggressive regularization using high dropout rates (25-50\%) and L2 weight penalties data augmentation through temporal windowing that generates multiple samples per experimental run; transfer learning where models pre-trained on large resting-state fMRI datasets are fine-tuned for emotion tasks and architecture design favoring parameter efficiency through global average pooling rather than large fully-connected layers.

A second challenge involves ensuring subject-independent generalization. Clinical and commercial applications require models that perform well on completely novel individuals not seen during training. However, substantial inter-subject variability in brain anatomy, functional organization, and emotional reactivity can cause models to overfit subject-specific idiosyncrasies. Rigorous evaluation through Leave-One-Subject-Out (LOSO) cross-valida-\\tion, where each subject is held out for testing while training on all others, provides unbiased estimates of true generalization performance. Studies employing LOSO validation have demonstrated that well-regularized CNNs can learn emotion representations that generalize across individuals, though performance typically decreases compared to subject-specific models.

A third challenge concerns interpretability: understanding what patterns the network has learned and whether they align with neuroscience knowledge. Techniques such as Grad-CAM (Gradient-weighted Class Activation Mapping) visualize which connections or brain regions most influence predictions by examining gradient flow through the network. Studies applying these visualization methods to emotion-decoding CNNs have found that learned patterns often align with established neuroscience highlighting limbic-prefrontal connections for emotion processing, default mode network for internal states, and attention networks for arousal modulation. This neurobiological plausibility provides confidence that models capture meaningful patterns rather than artifacts.

The integration of multiple brain parcellation atlases represents a particularly promising approach for enhancing CNN performance. Different anatomical atlases (Harvard-Oxford with 48 regions, AAL with 116 regions, Destrieux with 148 regions) provide complementary perspectives on brain organization at varying spatial scales. Coarse atlases capture large-scale network structure, while fine atlases reveal local connectivity patterns. Training separate CNN models on each atlas's connectivity matrices and combining their predictions through ensemble learning leverages this complementary information, improving accuracy beyond single-atlas approaches. This multi-scale integration mirrors the hierarchical organization of the brain itself, where emotion processing involves interactions spanning local circuits to large-scale networks.

Looking forward, deep learning for fMRI emotion recognition continues advancing through several directions. Recurrent neural networks (RNNs) and Long Short-Term Memory (LSTM) networks model temporal dynamics by processing fMRI time series sequentially, capturing evolving connectivity patterns during emotional episodes. Graph neural networks incorporate brain network topology explicitly, learning representations that respect anatomical structure. Attention mechanisms enable models to dynamically focus on relevant brain regions for each emotion category. Multi-modal integration combines fMRI with EEG for temporal precision, physiological signals for bodily state, and behavioral measures for comprehensive emotion assessment.

These advances position deep learning as a transformative technology for brain decoding applications. By achieving superior accuracy through automatic feature learning, capturing complex patterns beyond classical methods' reach, and scaling effectively as neuroimaging datasets grow, deep learning systems promise to accelerate neuroscience research, enable objective clinical diagnostics for affective disorders, and power next generation brain-computer interfaces for emotion-aware technology. This project contributes to this vision by developing a CNN-based system integrating multi-atlas connectivity for robust emotion recognition from fMRI data.